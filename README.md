# ğŸ¤– Conversational Bot - Client-Server Architecture

A bilingual (Romanian/English) voice-controlled conversational bot with a **client-server architecture** that allows distributing processing across multiple machines.

---

## ğŸ“‹ Overview

This project implements a voice assistant that can:
- ğŸ¤ Listen for wake words ("hello robot")
- ğŸ§ Transcribe speech to text (ASR)
- ğŸ§  Generate intelligent responses (LLM)
- ğŸ”Š Speak responses naturally (TTS)
- ğŸ›‘ Handle interruptions ("stop robot", "goodbye robot")
- ğŸ’¬ **Stream LLM responses** for faster perceived latency
- ğŸ§  **Maintain conversation history** (context-aware responses)
- ğŸŒ **Auto web search** (compound-beta model decides when to search)
- ğŸ¤– **Motor command integration** via tagged responses `[MOTOR:action:param]`
- ğŸ˜Š **Sentiment detection** and proactive suggestions
- ğŸ’¾ **TTS caching** for instant playback of common phrases

### Architecture Modes

|        Mode       |             Description                   |
|-------------------|-------------------------------------------|
| **Local**         | All processing on one machine             |
| **Client-Server** | Audio I/O on client, processing on server |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       CLIENT            â”‚   HTTP  â”‚        SERVER           â”‚
â”‚                         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                         â”‚
â”‚  ğŸ¤ Audio Capture       â”‚         â”‚  ğŸ§ ASR (Whisper)       â”‚
â”‚  ğŸ‘‚ Wake Word Detection â”‚         â”‚  ğŸ§  LLM (Groq/Ollama)   â”‚
â”‚  ğŸ”Š Audio Playback      â”‚         â”‚  ğŸ—£ï¸ TTS (Edge TTS)      â”‚
â”‚  ğŸ›‘ Stop Keyword        â”‚         â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Ubuntu 22.04/24.04 (or compatible Linux)
- Microphone and speakers
- Internet connection (for Groq LLM and Edge TTS)

### Installation

```bash

python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Set API key for Groq
echo "GROQ_API_KEY=your_key_here" > .env
```

### Running in Local Mode

```bash
# Set configs to local mode (in configs/*.yaml):
# mode: local

source .venv/bin/activate
LOG_LEVEL=INFO python -m src.app
```

### Running in Client-Server Mode

**Terminal 1 - Server:**
```bash
source .venv/bin/activate
python -m src.server.api --host 0.0.0.0 --port 8001
```

**Terminal 2 - Client:**
```bash
# Set configs to remote mode (in configs/*.yaml):
# mode: remote
# remote_host: "localhost"  # or server IP
# remote_port: 8001

source .venv/bin/activate
LOG_LEVEL=INFO python -m src.app
```

---

## ğŸ“ Project Structure

```
Conversational_Bot/
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â”œâ”€â”€ asr.yaml               # ASR settings (Whisper)
â”‚   â”œâ”€â”€ llm.yaml               # LLM settings (Groq/Ollama)
â”‚   â”œâ”€â”€ tts.yaml               # TTS settings (Edge TTS)
â”‚   â”œâ”€â”€ audio.yaml             # Audio & barge-in settings
â”‚   â””â”€â”€ wake.yaml              # Wake word settings
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                 # ğŸ¯ Main client application
â”‚   â”‚
â”‚   â”œâ”€â”€ server/                # ğŸ–¥ï¸ Server API
â”‚   â”‚   â”œâ”€â”€ api.py             # Flask REST endpoints
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ asr/                   # ğŸ§ Speech-to-Text
â”‚   â”‚   â”œâ”€â”€ interface.py       # ASRInterface, LocalASR, RemoteASR
â”‚   â”‚   â”œâ”€â”€ engine_faster.py   # Faster-Whisper implementation
â”‚   â”‚   â””â”€â”€ __init__.py        # Factory: make_asr()
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                   # ğŸ§  Language Model
â”‚   â”‚   â”œâ”€â”€ interface.py       # LLMInterface, LocalLLM, RemoteLLM
â”‚   â”‚   â”œâ”€â”€ engine.py          # Groq/Ollama/OpenAI implementation
â”‚   â”‚   â””â”€â”€ __init__.py        # Factory: make_llm()
â”‚   â”‚
â”‚   â”œâ”€â”€ tts/                   # ğŸ”Š Text-to-Speech
â”‚   â”‚   â”œâ”€â”€ interface.py       # TTSInterface, LocalTTS, RemoteTTS
â”‚   â”‚   â”œâ”€â”€ edge_backend.py    # Microsoft Edge TTS
â”‚   â”‚   â”œâ”€â”€ engine.py          # Piper/pyttsx3 fallback
â”‚   â”‚   â””â”€â”€ __init__.py        # Factory: make_tts()
â”‚   â”‚
â”‚   â”œâ”€â”€ audio/                 # ğŸ¤ Audio processing
â”‚   â”‚   â”œâ”€â”€ input.py           # Audio recording
â”‚   â”‚   â”œâ”€â”€ barge.py           # Barge-in detection
â”‚   â”‚   â”œâ”€â”€ vad.py             # Voice Activity Detection
â”‚   â”‚   â””â”€â”€ stop_keyword_detector.py
â”‚   â”‚
â”‚   â”œâ”€â”€ wake/                  # ğŸ‘‚ Wake word detection
â”‚   â”‚   â””â”€â”€ openwakeword_engine.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                  # âš™ï¸ Core utilities
â”‚   â”‚   â”œâ”€â”€ config.py          # Config loader
â”‚   â”‚   â”œâ”€â”€ logger.py          # Logging setup
â”‚   â”‚   â””â”€â”€ fast_exit.py       # Goodbye detection
â”‚   â”‚
â”‚   â””â”€â”€ telemetry/             # ğŸ“Š Metrics
â”‚       â””â”€â”€ metrics.py         # Prometheus metrics
â”‚
â”œâ”€â”€ voices/                    # ONNX voice models
â”‚   â”œâ”€â”€ hello_robot.onnx       # Wake word model
â”‚   â”œâ”€â”€ goodbye_robot.onnx     # Goodbye detection
â”‚   â””â”€â”€ stop_keyword.onnx      # Stop command model
â”‚
â”œâ”€â”€ models/                    # ASR models (Whisper)
â”œâ”€â”€ tools/                     # Utility scripts
â””â”€â”€ requirements.txt           # Python dependencies
```

---

## âš™ï¸ Configuration

### Client-Server Mode Settings

Each module (ASR, LLM, TTS) can be configured independently in their YAML files:

```yaml
# configs/asr.yaml
mode: remote                # local | remote
remote_host: "192.168.1.100"
remote_port: 8001
remote_timeout: 30.0
```

### Key Configuration Files

|          File        |                 Description            |
|----------------------|----------------------------------------|
| `configs/asr.yaml`   | Whisper model size, language, mode     |
| `configs/llm.yaml`   | Provider (groq/ollama), model, prompts |
| `configs/tts.yaml`   | Voice selection, caching, mode         |
| `configs/audio.yaml` | VAD, barge-in thresholds, stop keyword |
| `configs/wake.yaml`  | Wake phrases, OpenWakeWord settings    |

---

## ğŸ”Œ Server API Endpoints

|       Endpoint      | Method |         Description             |
|---------------------|--------|---------------------------------|
| `/health`           |   GET  | Server health check             |
| `/transcribe`       |   POST | Transcribe audio (WAV â†’ text)   |
| `/transcribe_ro_en` |   POST | Bilingual transcription (RO/EN) |
| `/generate`         |   POST | Generate LLM response           |
| `/generate_stream`  |   POST | Stream LLM tokens               |
| `/synthesize`       |   POST | Synthesize speech (text â†’ MP3)  |

---

## ğŸ”§ Technologies Used

|   Component   |              Technology              |
|---------------|--------------------------------------|
| **ASR**       | Faster-Whisper (Whisper optimized)   |
| **LLM**       | Groq Cloud (llama-3.3-70b) or Ollama |
| **TTS**       | Microsoft Edge TTS (Neural voices)   |
| **Wake Word** | OpenWakeWord (custom ONNX)           |
| **Server**    | Flask (REST API)                     |
| **Audio**     | sounddevice, WebRTC VAD              |

---

## âš¡ Advanced Features

### LLM Streaming
Responses are streamed token-by-token for instant feedback:
```python
# In configs/llm.yaml
streaming: true  # Enables token-by-token generation
```

### Conversation History
Maintains context across multiple turns:
```yaml
# In configs/llm.yaml
history_enabled: true
max_history_turns: 2  # Keeps last 2 user/assistant exchanges
```

### Auto Web Search (Compound-Beta)
The `compound-beta` model automatically searches the web when needed:
```yaml
# In configs/llm.yaml
model: "compound-beta"  # Auto web search when lacking info
```
âš ï¸ Note: compound-beta takes 3-8s due to web search vs. ~1s for llama-3.1-8b-instant

### Motor Command Integration
LLM can control robot motors via tagged responses:
```
User: "Raise your left hand"
Bot: "Ok, done. [MOTOR:raise_hand:left]"
```

Supported tags:
- `[MOTOR:raise_hand:left|right]` - Raise specified hand
- `[MOTOR:wave:left|right]` - Wave hand
- `[MOTOR:nod_head]` - Nod head
- `[INTENT:question]` - Indicates user asked a question
- `[INTENT:greeting]` - Greeting detection

### TTS Caching
Common phrases are pre-synthesized and cached for instant playback (\<100ms):
```yaml
# In configs/tts.yaml
cache_enabled: true
common_phrases:
  - "Hello!"
  - "I don't understand."
  - "Let me think about that."
```

### Sentiment Detection
Detects user sentiment and provides proactive suggestions:
- Positive sentiment â†’ Encouraging responses
- Negative sentiment â†’ Supportive tone
- Neutral â†’ Standard informative responses


---

## ğŸŒ Deployment on Two Machines

### Step 1: Clone on both machines

```bash
# On both laptops:
git clone https://github.com/Delia63/Conversational_Robot.git
cd Conversational_Robot/Conversational_Bot
git checkout client-server
pip install -r requirements.txt
```

### Step 2: Configure server (Laptop 2)

```bash
# Start server listening on all interfaces
python -m src.server.api --host 0.0.0.0 --port 8001
```

### Step 3: Configure client (Laptop 1)

Edit `configs/asr.yaml`, `configs/llm.yaml`, `configs/tts.yaml`:
```yaml
mode: remote
remote_host: "192.168.1.X"  # Replace with Laptop 2 IP
remote_port: 8001
```

```bash
# Start client
LOG_LEVEL=INFO python -m src.app
```
python -m src.server.api --host 127.0.0.1 --port 8001
---

## ğŸ“Š Performance Metrics

|    Metric       |         Typical Value      |
|-----------------|----------------------------|
| ASR Latency     | ~3-5s (Whisper small, CPU) |
| LLM First Token | ~200-300ms (Groq)          |
| Round-trip      | ~2-3s                      |
| TTS Cache Play  | <100ms                     |

Access metrics at: `http://localhost:9108/vitals`

---

## ğŸ¯ Voice Commands

### Wake Words (Start Conversation)
Say any of these to wake up the robot:

| Language | Command | Model |
|----------|---------|-------|
| English | "Hello robot" | `hello_robot` |
| English | "What's up buddy" | `wots_up_bud_dee` |
| English | "Listen up" | `lis_un_up` |
| Romanian | "BunÄƒ robot" | `boona_ro_bot` |
| Romanian | "Salut robot" | `sa_loot_ro_bot` |

### Session Commands

| Command | Action | Detection Method |
|---------|--------|------------------|
| **"Goodbye robot"** | End session and return to standby | OpenWakeWord (hotword) |
| **"Bye bye"** | End session (alternative) | OpenWakeWord (hotword) |
| **"Stop robot"** | Stop current TTS playback immediately | Stop Keyword Detector (ONNX) |

### How It Works
- **Wake words**: Always listening in standby mode via OpenWakeWord
- **Goodbye**: Active only during conversation session
- **Stop**: Active only during TTS playback (requires 2 consecutive detections for reliability)

---

## ğŸ”— Related Files

- [INSTALL.md](INSTALL.md) - Detailed installation guide
- [FEATURES.md](FEATURES.md) - Feature documentation
- [LIMITATIONS.md](LIMITATIONS.md) - Known limitations
